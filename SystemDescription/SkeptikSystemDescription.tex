\documentclass{llncs}

\usepackage{xcolor}
\usepackage{enumitem,amsmath,amssymb}
\usepackage{breakurl}    % used for \url and \burl
\usepackage[linesnumbered,boxed,noline,noend]{algorithm2e}
\def\defaultHypSeparation{\hskip.1in}

\usepackage{tikz}
\usepackage{subfig}
\usepackage{array,booktabs,multirow}
\usepackage{placeins}

\usepackage{logictools}
\usepackage{prooftheory}
\usepackage{comment}
\usepackage{mathenvironments}
\usepackage{drawproof}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}

\title{The \skeptik Proof Compression System}

\author{
  Joseph Boudou\inst{1}
  \thanks{Supported by the Google Summer of Code 2012 program.}
  \and 
  Andreas Fellner\inst{2}
  \thanks{Supported by the Google Summer of Code 2013 program.}
  \and 
  Bruno Woltzenlogel Paleo\inst{3}
  \thanks{Supported by the Austrian Science Fund, project P24300.}
}

\authorrunning{J.\~Boudou \and A. Fellner \and B.\~Woltzenlogel Paleo}

\institute{
  IRIT, Universit\'e de Toulouse, France \\
  \email{joseph.boudou@irit.fr}
  \and 
  Free University of Bolzano, Italy \\
  \email{fellner.a@gmail.com}
  \and 
  Vienna University of Technology, Austria \\
  \email{bruno@logic.at}
}




\begin{document}

\maketitle


\begin{abstract}
This paper describes \skeptik: a system for compressing and improving proofs obtained by automated reasoning tools. 
\end{abstract}

\setcounter{footnote}{0}


\section{Introduction}



\section{User Interface}



\section{Supported Proof Formats}

\subsection{TraceCheck Format}

\subsection{The \veriT Proof Format}

\subsection{The \skeptik Proof Format}



\section{Available Proof Compression Algorithms}


\subsection{RPI}

\subsection{LU}

\subsection{LUniv}

The \algo{LowerUnivalents} algorithm \cite{LUniv} (\algo{LUniv}) is a generalisation of
\algo{LowerUnits}. Taking into account the already lowered subproofs it becomes possible to lower a
non-unit node if all its conclusion's literals but one can be deleted by those already lowered
subproofs. Moreover, some partial regularisation based on the lowered pivots can be achieved
simultaneously.

In \skeptik this algorithm has been implemented as a replacement of the \algo{fixProof} function
used by some algorithms to reconstruct a proof after deletions. Therefore, non-sequential
combinations of those algorithms with \algo{LUniv} are easy to implement. For instance,
\algo{LUnivRPI} is a non-sequential combination of \algo{LUniv} after \algo{RPI}. This latter
algorithm is currently one the best tradeoff between compression time and compression ratio
\cite{LUniv}.

\subsection{RPI[3]LU and RPI[3]LUniv}

\subsection{RedRec}

\subsection{Split}

The Split \cite{CottonSplit} algorithm is a technique to lower pivot variables in a proof.\\
From a proof with conclusion $C$, two proofs with conclusion $v \vee C$ and $\neg{v} \vee C$ are constructed,  where the variable $v$ is chosen heuristically.\\
In a first step the positive/negative premises of resolvents with pivot $v$ are removed from the proof.
Afterwads the proof is fixed, by traversing it top-down and fixing each proof node.
A proof node is fixed by either replacing it by one of or resolving the fixed premises.\\ %I'm not sure if the end of this sentence is elegant or unreadable
The roots of the resulting proofs are resolved, using $v$ as pivot, to obtain a new proof of $C$.\\
The time-complexity of this algorithm is linear in the proof length %this should be true, right? but the original paper doesn't state this
and it suits very well for repeated application. This can be done iteratively or recursively. Also multiple variables can be chosen in advance. 
All these variants are implemented into Skeptik.

\subsection{TautologyElimination}

\subsection{StructuralHashing}

\subsection{DAGification}

\subsection{Subsumption}

Subsumption based algorithms use, as the name implies, the subsumption relation on clauses for compressing a proof. A clause $C_1$ subsumes a clause $C_2$ \textit{iff} every literal occuring in $C_1$ also occurs in $C_2$.
The general goal is to replace a clause $C$ by another clause $D$, used elsewhere in the proof, that are such that $C$ subsumes $D$.
There are three subsumption based compression algorithms implemented into Skeptik.\\
\textbf{Top-down Subsumption} searches for subsumed clauses among all clauses visited earlier in a top-down traversal. The time-complexity of this algorithm is worst case quadratic in the number of proof nodes.\\
\textbf{Bottom-up Subsumption} searches for subsumed clauses among all clauses visited earlier in a bottom-up traversal. A subsumed clause $D$ can only replace a clause $C$, if $D$ is not an ancestor of $C$ in the graph representing the proof. Bottom-up Subsumption has the same time-complexity as Top-down Subsumption, but the additional ancestor-check makes it slower in practise.\\
\textbf{RecycleUnits} \cite{RP11} is a special case of Bottom-up Subsumption that only searches for subsumed clauses that are unit (i.e. contain only one literal). This algorithm has a worst case time-complexity that is quadratic in the number of unit clauses.

\subsection{Pebbling}

Pebbling algorithms compress proofs in the \textbf{space measure}, as opposed to the usual length compression. The space measure of a proof indicates how many proof nodes maximally have to be kept in memory, while reading the proof, simultaniously.\\
This measure is closely related to the \textbf{Black Pebbling Game} \cite{gilbert1980pebbling}, which lends its name to the algorithms described here. A strategy for this game directly corresponds to a topological node ordering, i.e. an ordering of nodes such that every premise of each node is lower than the node itself, combined with deletion information, i.e. extra lines in the proof output indicating that a node can be deleted from memory. An optimal strategy for the Black Pebbling Game would therefore result in optimal space compression of the proof. However, as shown in \cite{gilbert1980pebbling}, finding the optimal solution is PSPACE-complete and therefore not a feasable approach for this program.\\
To obtain algorithms that have an acceptable runtime greedy heuristics for finding a good node ordering are used.\\
\textbf{Top-down pebbling} directly corresponds to playing the game with limited information on how the proof looks. At every point there are nodes which can be pebbled (initially these are only the axioms). Using information from these nodes and their children nodes, it is decided which node to pebble next, which can make other nodes pebblable. This is done until the root node is pebbled. Unfortunately the lack of knowledge about the structure of the proof often results in bad space compression.\\
\textbf{Bottom-up pebbling} constructs a node ordering by visiting proof nodes and their premises recursively, starting from the root node. At every node it is chosen heuristically in what order its premises are visited. After all premises are visited, the node is added to the oder.

\section{Implementation Details}

ToDo: statistics about the code

\subsection{Organization of the Code}

ToDo: brief description of the package structure


\subsection{Data Structures for Formulas}


\subsection{Data Structures for Proofs}




\section{Conclusions and Future Work}

ToDo: limitation: memory consumption in Scala
underlying symbols are strings



\bibliographystyle{splncs}
\bibliography{biblio}


\end{document}

% vim: tw=100
